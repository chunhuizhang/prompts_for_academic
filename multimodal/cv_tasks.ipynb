{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2473a521-a556-4f5e-b14b-6928de51d95f",
   "metadata": {},
   "source": [
    "- Conversational image segmentation with Gemini 2.5\n",
    "    - https://developers.googleblog.com/en/conversational-image-segmentation-gemini-2-5/\n",
    "        - online: https://aistudio.google.com/app/apps/bundled/spatial-understanding?showPreview=true&appParams=task%3Dsegmentation-masks\n",
    "    - best practices\n",
    "        - 1: Use the gemini-2.5-flash model\n",
    "        - 2: Disable thinking set (thinkingBudget=0)\n",
    "        - 3: Stay close to the recommended prompt, and request JSON as output format.\n",
    "    \n",
    "```\n",
    "# segmentation task\n",
    "Give the segmentation masks for the objects. \n",
    "Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\". \n",
    "Use descriptive labels.\n",
    "\n",
    "# points task\n",
    "Point to the items with no more than 10 items. The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>}, ...]. The points are in [y, x] format normalized to 0-1000.\n",
    "\n",
    "# 2d bbox\n",
    "Detect items, with no more than 20 items. Output a json list where each entry contains the 2D bounding box in \"box_2d\" and a text label in \"label\".\n",
    "\n",
    "# 3d bbox\n",
    "Output in json. Detect the 3D bounding boxes of items, output no more than 10 items. Return a list where each entry contains the object name in \"label\" and its 3D bounding box in \"box_3d\".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f8e08-bcf4-4e58-b252-008127cd4698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba07586-41da-411e-9a88-59f6ee510145",
   "metadata": {},
   "source": [
    "- https://github.com/john-carroll-sw/gpt-4o-object-detection-in-image-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e42cfcb-ba28-43bf-bf24-2ba323291ea4",
   "metadata": {},
   "source": [
    "```\n",
    "Given an image, identify and describe all the objects present. \n",
    "Provide a detailed description of each object, including its position in the image, apparent size, color, and any other notable features. \n",
    "Additionally, if possible, provide context about the scene or environment the objects are in. The description should be clear and concise, suitable for further processing or analysis.\n",
    "Return json.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efabb7-284c-4de5-9e49-10a7aff7ccfc",
   "metadata": {},
   "source": [
    "- qwen vl: https://qwenlm.github.io/blog/qwen2.5-vl/\n",
    "\n",
    "```\n",
    "Detect all motorcyclists in the image and return their locations in the form of coordinates. The format of output should be like {“bbox_2d”: [x1, y1, x2, y2], “label”: “motorcyclist”, “sub_label”: “wearing helmat” # or “not wearing helmat”}.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
