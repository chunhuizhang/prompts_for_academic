{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d92afbf-955c-41ef-83fb-acb501c4a16c",
   "metadata": {},
   "source": [
    "- url\n",
    "    - 网页\n",
    "    - 图片\n",
    "    - pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2376c-9276-4243-ae85-1ecfb42b4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "assert load_dotenv()\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents='''详细地总结这个课件内容，用latex数学语言描述，中文回答\n",
    "https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-6.pdf''',\n",
    "    config={\"tools\": [{\"url_context\": {}}]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12eafad9-7f75-428a-a700-4e5ac5741525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context used: https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-6.pdf\n"
     ]
    }
   ],
   "source": [
    "print(f\"Context used: {', '.join([url.retrieved_url for url in response.candidates[0].url_context_metadata.url_metadata])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95e964a-6bab-4181-a3de-851c074fe681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "该课件《Actor-Critic Algorithms - CS 285》详细介绍了策略梯度（policy gradients）算法的变种——Actor-Critic算法。\n",
      "\n",
      "课件首先回顾了策略梯度算法的基本思想，即通过生成样本、估计回报、然后改进策略的迭代过程。策略梯度的更新公式为：\n",
      "$$ \\nabla_{\\theta} J(\\theta) \\approx \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) \\hat{Q}_t \\right] $$\n",
      "其中，$\\hat{Q}_t$是折扣回报（\"reward to go\"）的估计。\n",
      "\n",
      "为了降低策略梯度估计的方差，课件引入了基线（baseline）的概念，指出最优的基线是状态值函数$V^{\\pi}(s_t)$。Actor-Critic算法的核心思想是将策略梯度方法与值函数估计结合起来。\n",
      "\n",
      "**Actor-Critic算法构成**:\n",
      "*   **Actor（策略网络）**：负责学习策略$\\pi_{\\theta}(a|s)$，用于生成动作。其更新目标是最大化预期的回报，并利用Critic提供的优势函数（advantage function）来指导策略更新。\n",
      "*   **Critic（评论员网络）**：负责学习值函数，通常是状态值函数$V^{\\pi}(s)$或状态-动作值函数$Q^{\\pi}(s,a)$。Critic的目标是准确估计当前策略下的值函数，为Actor提供一个基线，从而降低策略梯度估计的方差。\n",
      "\n",
      "**优势函数（Advantage Function）**:\n",
      "Actor-Critic算法通常使用优势函数$A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$来更新策略，策略梯度更新可以写为：\n",
      "$$ \\nabla_{\\theta} J(\\theta) \\approx \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) A^{\\pi}(s_t,a_t) \\right] $$\n",
      "使用优势函数可以消除回报估计中的偏差，同时降低方差。\n",
      "\n",
      "**Actor-Critic算法设计选择**:\n",
      "*   **网络架构**:\n",
      "    *   **双网络设计（Two-network design）**：Actor和Critic使用独立的神经网络。这种设计简单稳定，但Actor和Critic之间不共享特征。\n",
      "    *   **共享网络设计（Shared network design）**：Actor和Critic共享底层的特征提取层，但有不同的输出头。这种设计可以促进特征共享，但在实践中可能需要更精细的调优。\n",
      "*   **在线与批量模式**:\n",
      "    *   **在线（Online）Actor-Critic**：策略在每个时间步或小批量数据后更新。常见的并行实现包括同步并行Actor-Critic和异步并行Actor-Critic（如A3C）。\n",
      "    *   **批量（Batch-mode）Actor-Critic**：收集大量样本后进行策略更新，如TRPO或PPO通常是批量模式。\n",
      "*   **离策略（Off-policy）Actor-Critic**: 允许使用回放缓冲区（replay buffer）中的旧经验进行训练，打破了On-policy的限制。但需要解决离策略训练带来的偏差问题，通常通过重要性采样（importance sampling）或更复杂的算法（如SAC）来解决。\n",
      "\n",
      "**方差降低技术**:\n",
      "课件还讨论了进一步降低方差的方法，如：\n",
      "*   **n步回报（n-step returns）和资格迹（Eligibility Traces）**: 在单步TD和蒙特卡洛方法之间进行权衡，通过调整$n$来控制偏差与方差的平衡。\n",
      "*   **广义优势估计（Generalized Advantage Estimation, GAE）**: GAE是一种融合了n步回报的优势函数估计方法，通过加权组合不同n步回报来获得更优的偏差-方差权衡。GAE的优势函数定义为：\n",
      "    $$ \\hat{A}_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} $$\n",
      "    其中$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$是TD残差。\n",
      "\n",
      "总而言之，Actor-Critic算法通过结合值函数估计来降低策略梯度算法的方差，从而提高学习效率和稳定性。课件还提供了Actor-Critic算法的实例，如A3C和基于GAE的高维连续控制算法。\n"
     ]
    }
   ],
   "source": [
    "# print the response\n",
    "print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650dc8d-88c9-44a6-9969-77860b24cb45",
   "metadata": {},
   "source": [
    "课件首先回顾了策略梯度算法的基本思想，即通过生成样本、估计回报、然后改进策略的迭代过程。策略梯度的更新公式为：\n",
    "$$ \\nabla_{\\theta} J(\\theta) \\approx \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) \\hat{Q}_t \\right] $$\n",
    "其中，$\\hat{Q}_t$是折扣回报（\"reward to go\"）的估计。\n",
    "\n",
    "为了降低策略梯度估计的方差，课件引入了基线（baseline）的概念，指出最优的基线是状态值函数$V^{\\pi}(s_t)$。Actor-Critic算法的核心思想是将策略梯度方法与值函数估计结合起来。\n",
    "\n",
    "**Actor-Critic算法构成**:\n",
    "*   **Actor（策略网络）**：负责学习策略$\\pi_{\\theta}(a|s)$，用于生成动作。其更新目标是最大化预期的回报，并利用Critic提供的优势函数（advantage function）来指导策略更新。\n",
    "*   **Critic（评论员网络）**：负责学习值函数，通常是状态值函数$V^{\\pi}(s)$或状态-动作值函数$Q^{\\pi}(s,a)$。Critic的目标是准确估计当前策略下的值函数，为Actor提供一个基线，从而降低策略梯度估计的方差。\n",
    "\n",
    "**优势函数（Advantage Function）**:\n",
    "Actor-Critic算法通常使用优势函数$A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$来更新策略，策略梯度更新可以写为：\n",
    "$$ \\nabla_{\\theta} J(\\theta) \\approx \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) A^{\\pi}(s_t,a_t) \\right] $$\n",
    "使用优势函数可以消除回报估计中的偏差，同时降低方差。\n",
    "\n",
    "**Actor-Critic算法设计选择**:\n",
    "*   **网络架构**:\n",
    "    *   **双网络设计（Two-network design）**：Actor和Critic使用独立的神经网络。这种设计简单稳定，但Actor和Critic之间不共享特征。\n",
    "    *   **共享网络设计（Shared network design）**：Actor和Critic共享底层的特征提取层，但有不同的输出头。这种设计可以促进特征共享，但在实践中可能需要更精细的调优。\n",
    "*   **在线与批量模式**:\n",
    "    *   **在线（Online）Actor-Critic**：策略在每个时间步或小批量数据后更新。常见的并行实现包括同步并行Actor-Critic和异步并行Actor-Critic（如A3C）。\n",
    "    *   **批量（Batch-mode）Actor-Critic**：收集大量样本后进行策略更新，如TRPO或PPO通常是批量模式。\n",
    "*   **离策略（Off-policy）Actor-Critic**: 允许使用回放缓冲区（replay buffer）中的旧经验进行训练，打破了On-policy的限制。但需要解决离策略训练带来的偏差问题，通常通过重要性采样（importance sampling）或更复杂的算法（如SAC）来解决。\n",
    "\n",
    "**方差降低技术**:\n",
    "课件还讨论了进一步降低方差的方法，如：\n",
    "*   **n步回报（n-step returns）和资格迹（Eligibility Traces）**: 在单步TD和蒙特卡洛方法之间进行权衡，通过调整$n$来控制偏差与方差的平衡。\n",
    "*   **广义优势估计（Generalized Advantage Estimation, GAE）**: GAE是一种融合了n步回报的优势函数估计方法，通过加权组合不同n步回报来获得更优的偏差-方差权衡。GAE的优势函数定义为：\n",
    "    $$ \\hat{A}_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} $$\n",
    "    其中$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$是TD残差。\n",
    "\n",
    "总而言之，Actor-Critic算法通过结合值函数估计来降低策略梯度算法的方差，从而提高学习效率和稳定性。课件还提供了Actor-Critic算法的实例，如A3C和基于GAE的高维连续控制算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8b43d-e78e-4de1-b0e8-868d3cd71aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "verl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
