{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c1115b-9dd1-4544-98a5-846b6d0c9350",
   "metadata": {},
   "source": [
    "- https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\n",
    "- MT-Bench：\n",
    "    - 构成：一个包含80个高质量、多轮对话问题的数据集。\n",
    "    - 特点：涵盖了写作、角色扮演、推理、数学、编程等8个常见类别。每个问题都包含两轮对话，旨在测试模型在多轮交互中遵循指令的能力。\n",
    "    - 用途：作为一个标准化的、可控的测试环境，用于精确比较不同模型的能力。\n",
    "- Types of LLM-as-a-Judge\n",
    "    - Single Answer Grading\n",
    "    - Pairwise Comparison\n",
    "    - Reference-guided Grading\n",
    "- 裁判LLM的局限性与偏见 (Limitations)\n",
    "    - 位置偏见 (Position Bias)：裁判LLM倾向于偏爱第一个呈现给它的答案。实验表明，即使交换两个完全相同的答案的顺序，裁判的判断也可能发生改变。GPT-4虽然也存在此问题，但一致性（超过60%）远高于其他模型。\n",
    "    - 冗长偏见 (Verbosity Bias)：裁判LLM倾向于偏爱更长、更详细的回答，即使这些回答包含不必要的重复信息。\n",
    "    - 自我增强偏见 (Self-enhancement Bias)：裁判LLM可能倾向于偏爱由其自身或同系列模型生成的答案。例如，GPT-4作为裁判时，对GPT-4模型回答的胜率有约10%的提升。\n",
    "    - 有限的推理与数学能力：在评判复杂的推理或数学问题时，即使裁判LLM自己有能力解决这个问题，它也容易被模型给出的错误答案误导，从而做出错误的判断。\n",
    "- 应对偏见和局限性的解决方案\n",
    "    - 交换位置 (Swapping Positions)：为了克服位置偏见，可以将两个模型的答案顺序交换后，再让裁判评估一次。只有在两种顺序下都判定同一个模型获胜时，才算作有效胜利，否则视为平局。\n",
    "    - 思维链与参考引导 (Chain-of-Thought & Reference-guided)：为了提升评判数学和推理问题的准确性，可以采用两种策略：\n",
    "        - CoT：在提示中要求裁判“首先独立地、一步一步地解决问题，然后再比较两个助手的答案”。\n",
    "        - 参考引导：先让裁判LLM生成一个正确答案，然后将这个答案作为“参考”提供给它，再让它进行评判。实验证明，参考引导的方法效果显著，能将数学问题的评判失败率从70%降低到15%。\n",
    "    - 处理多轮对话的提示设计：在评判多轮对话时，将完整的对话历史（两轮问答）作为一个整体输入给裁判LLM，比将两轮对话拆分成两个独立的提示要好得多。这能避免裁判因上下文缺失而做出错误判断。\n",
    "- conclusion\n",
    "    - 与人类高度一致：强大的LLM裁判（特别是GPT-4）与人类专家的判断一致率超过80%。这个水平与人类之间（不同的人类专家之间）的判断一致率相当。这证明了LLM-as-a-Judge作为人类评估代理的可行性。\n",
    "    - GPT-4是目前最佳裁判：与其他模型（如GPT-3.5, Claude）相比，GPT-4作为裁判时表现出更高的一致性、更少的偏见和更强的判断能力。\n",
    "        - LLM裁判的判断与“黄金标准”——人类专家的判断——进行直接比较。\n",
    "        - 我们将两种裁判类型之间的一致性定义为，从每种类型中随机选择的（但非同一的）个体对一个随机选择的问题达成一致意见的概率。\"\n",
    "    - 差距越大，判断越准：当两个被评估模型的性能差距很大时，LLM裁判与人类的判断一致率接近100%。当两个模型性能接近时，一致率会下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a9cfa5-c4d7-4cf6-8cca-cecf96731f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c75a49-d709-4865-9468-c7d548d9d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = defaultdict(dict)\n",
    "with open('./data/judge_prompts.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        prompt = json.loads(line)\n",
    "        prompts[prompt['name']] = prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a605ff6-c5dd-4231-ac1e-e18ae46a8fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33e5e4a-7908-488f-a14c-e9d1033c4ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'pair-v2',\n",
       " 'type': 'pairwise',\n",
       " 'system_prompt': 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.',\n",
       " 'prompt_template': \"[User Question]\\n{question}\\n\\n[The Start of Assistant A's Answer]\\n{answer_a}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{answer_b}\\n[The End of Assistant B's Answer]\",\n",
       " 'description': 'Prompt for general questions',\n",
       " 'category': 'general',\n",
       " 'output_format': '[[A]]'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts['pair-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea8a5a0-2e14-4ac6-ab6e-812df4a64683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair-v2 pairwise Prompt for general questions general [[A]]\n",
      "pair-v2-multi-turn pairwise Prompt for multi-turn general questions general [[A]]\n",
      "pair-math-v1 pairwise Prompt for math questions math [[A]]\n",
      "pair-math-v1-multi-turn pairwise Prompt for multi-turn general questions general [[A]]\n",
      "single-v1 single Prompt for general questions general [[rating]]\n",
      "single-math-v1 single Prompt for general questions math [[rating]]\n",
      "single-v1-multi-turn single Prompt for general questions general [[rating]]\n",
      "single-math-v1-multi-turn single Prompt for general questions math [[rating]]\n"
     ]
    }
   ],
   "source": [
    "for name, prompt in prompts.items():\n",
    "    print(name, prompt['type'], prompt['description'], prompt['category'], prompt['output_format'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907e4be-e6e2-4c56-a09d-b3ccb19ed61c",
   "metadata": {},
   "source": [
    "### single-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f528d0e4-480a-41d5-827a-23acd8a5caea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'single-v1',\n",
       " 'type': 'single',\n",
       " 'system_prompt': 'You are a helpful assistant.',\n",
       " 'prompt_template': '[Instruction]\\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant\\'s Answer]\\n{answer}\\n[The End of Assistant\\'s Answer]',\n",
       " 'description': 'Prompt for general questions',\n",
       " 'category': 'general',\n",
       " 'output_format': '[[rating]]'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts['single-v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b92280e-c043-4cd8-9963-070243977f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instruction]\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "\n",
      "[Question]\n",
      "{question}\n",
      "\n",
      "[The Start of Assistant's Answer]\n",
      "{answer}\n",
      "[The End of Assistant's Answer]\n"
     ]
    }
   ],
   "source": [
    "# {question}, {answer}\n",
    "print(prompts['single-v1']['prompt_template'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7efa5cd5-9896-49b1-ac4b-51d54b94d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instruction]\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "\n",
      "[Question]\n",
      "{question}\n",
      "\n",
      "[The Start of Reference Answer]\n",
      "{ref_answer_1}\n",
      "[The End of Reference Answer]\n",
      "\n",
      "[The Start of Assistant's Answer]\n",
      "{answer}\n",
      "[The End of Assistant's Answer]\n"
     ]
    }
   ],
   "source": [
    "print(prompts['single-math-v1']['prompt_template'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d0619-486e-46f1-9277-82d40807be3a",
   "metadata": {},
   "source": [
    "### pair-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da80b11-1cb3-466f-980c-e93eeb87c996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'pair-v2',\n",
       " 'type': 'pairwise',\n",
       " 'system_prompt': 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.',\n",
       " 'prompt_template': \"[User Question]\\n{question}\\n\\n[The Start of Assistant A's Answer]\\n{answer_a}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{answer_b}\\n[The End of Assistant B's Answer]\",\n",
       " 'description': 'Prompt for general questions',\n",
       " 'category': 'general',\n",
       " 'output_format': '[[A]]'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts['pair-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac1b3f26-967f-48cb-94d2-69fa360b38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.\n"
     ]
    }
   ],
   "source": [
    "print(prompts['pair-v2']['system_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c371e988-66ec-44ab-9a3f-ef25494996c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[User Question]\n",
      "{question}\n",
      "\n",
      "[The Start of Assistant A's Answer]\n",
      "{answer_a}\n",
      "[The End of Assistant A's Answer]\n",
      "\n",
      "[The Start of Assistant B's Answer]\n",
      "{answer_b}\n",
      "[The End of Assistant B's Answer]\n"
     ]
    }
   ],
   "source": [
    "# {question}, {answer_a}, {answer_b}\n",
    "print(prompts['pair-v2']['prompt_template'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e357e15d-313e-4c30-8978-ca63051179d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A's answer, and assistant B's answer. Your job is to evaluate which assistant's answer is better. Begin your evaluation by comparing both assistants' answers with the reference answer. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.\n"
     ]
    }
   ],
   "source": [
    "print(prompts['pair-math-v1']['system_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a221ad2c-06c1-4144-addf-6d9a552dfb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[User Question]\n",
      "{question}\n",
      "\n",
      "[The Start of Reference Answer]\n",
      "{ref_answer_1}\n",
      "[The End of Reference Answer]\n",
      "\n",
      "[The Start of Assistant A's Answer]\n",
      "{answer_a}\n",
      "[The End of Assistant A's Answer]\n",
      "\n",
      "[The Start of Assistant B's Answer]\n",
      "{answer_b}\n",
      "[The End of Assistant B's Answer]\n"
     ]
    }
   ],
   "source": [
    "print(prompts['pair-math-v1']['prompt_template'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc807a-23cc-40f9-9aef-b4ac87a1a12e",
   "metadata": {},
   "source": [
    "### multi-turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb7dabae-f86e-4a27-9b8e-948625bdb44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You evaluation should focus on the assistant's answer to the second user question. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts['single-v1-multi-turn']['system_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11933095-7512-4892-b40c-29b7837561c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|The Start of Assistant A's Conversation with User|>\n",
      "\n",
      "### User:\n",
      "{question_1}\n",
      "\n",
      "### Assistant A:\n",
      "{answer_1}\n",
      "\n",
      "### User:\n",
      "{question_2}\n",
      "\n",
      "### Assistant A:\n",
      "{answer_2}\n",
      "\n",
      "<|The End of Assistant A's Conversation with User|>\n"
     ]
    }
   ],
   "source": [
    "print(prompts['single-v1-multi-turn']['prompt_template'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb27d85-8ecc-488e-a0fd-356eef6ef4e9",
   "metadata": {},
   "source": [
    "### chat with gemini 2.5 pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a20da6-362c-443c-a212-e16b5f7fa7c4",
   "metadata": {},
   "source": [
    "- gemini 2.5 pro\n",
    "    - 原生多模态，原生 pdf 输入，\n",
    "    - 精准 attention（long context 能力）\n",
    "- 带着问题，query；\n",
    "- 不断追问；\n",
    "- 定位到 paper 相关 section；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
